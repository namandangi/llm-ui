## Build Instructions

#### 1. First clone the repository locally by

```
    git clone https://github.com/namandangi/gptzero-fs-assessment-2024.git
```

and change directory into the project using

```
    cd ./gptzero-fs-assessment-2024.git
```

#### 2. Install all packages

From the root directory of the project using

```
cd ./frontend
npm install 

cd ../backend
npm install

cd../richeRich
npm install
```

#### 3. Create a .env file for your backend (optional)

From root directory go to your backend service 

```
cd ./backend
touch .env
cp -v .env.example .env 
```

Fill out the appropriate environment values, you can refer to default config values in backend using 

```
cat ./config/constants.js
``` 

#### 4. Run all the servers

From the root directory of the project using


(Open a new terminal session for each service)

```
cd ./frontend
npm run dev

```
   
```
cd ./richieRich
npm run start
```

```
cd ./backend
npm run start
```

Answer to Questions:

1. If you were given 2 weeks time, what changes would you make to the project to enhance code quality, UX, performance, or scalability.

    - All UX and code structuring related criteria implemented over the week and high-level thoughts around scale and architecture documented below.  
    
2. Anything you'd like us to know about your submission? 

    - All the key features such as debouncing, class based components, UI optimization, unit tests, UX imporvements, exponential backoff re-connection for Websocket clients, etc. addressed below. 

### Demo:
 
![](https://github.com/namandangi/gptzero-fs-assessment-2024/blob/master/static/demo.gif)

### Features:

    1. Initially when the user has not entered any prompt, the submit button is grayed out and displays an error when user tries to click on the diabled button
    2. When the user enters a prompt, the button becomes active and changes color and allows submitting 
    3. The prompt generated by the API is streamed through backend to frontend via websockets.
    4. We controll the rate at which the UI updates using debouncing to make sure there is a delay of 15ms between each text render making the UI flow much smoother
    5. The UI components are optimized so that not all the components are re rendered each time a new token comes in.
    6. Backend is modularized into class based functions, to keep the code cleaner and maintainable.
    7. Unit tests included to test basic connection and message passing functionalties for ws server and client, right now unit tests do not include end-to-end message passing tests, since API could be an external service and the output being probabilistic, more research is required into testing such functionality.
    8. Added exponential backoff re-connection for Websocket clients.   


## UX, Architecture and Scale Considerations


![](https://github.com/namandangi/gptzero-fs-assessment-2024/blob/master/static/arch.jpeg)


**Architecture:** 

    I'm still thinking if a 2 way websocket connection is the best way to go or if we could perhaps just get away with long polling/server sent event.
     - Websockets are more complex to setup than SSEs (supported by most browsers) but it makes more sense to use websockets since we want to establish a bi-directional communication and SSEs are mainly used for server pushed events, so in cases like stock market price updates or IOT sensor data visualization would be a better use case for SSEs but chat like applications are better suited for websockets.

    We can also introduce a cache layer like Redis to store client and API sessions and even persist the chat history in a document database, to preserve related context. 
    - Right now, I'm storing clients in memory, we can easily introduce redis here directly in WS server class' constructor initialization with minimum friction

**Scale:** 

    My initial thoughts were, 

    1. Are we relying on external services for the LLM/GPT APIs or is it our own in-house service. T
    his could be important becuase we can easily put up a load balancer between the client layers and server layers and call it day 
    if we don't directly manage the API services. However, if we are to also manage it, 
    then we have to probably introduce another load balancer between backend service and the API service. 
    
    2. Initial ideas around load balancing would be that since the websockets are established over TCP connections,
    an L4 loadbalancer could handle the balancing and connection management for us between our frontend and backend API services, however, 
    it might be a bit tricky to maintain session between the API server and backend service.
    For example, if the backend service fails before it completely receives all the tokens from the API server, 
    how would we handle such a scenario? Since, the richeRich service checks for the done/end token to terminate ws session, 
    we can implement a similar token/checksum mechanism with a retry on the frontend but that feels more like a business decision (SLA/SLO types) right now.
    With the load balancing mechanism we can guarantee high availability and partition tolerance 
    but consistency still remains questionable due to potential failures. Furthermore, for persistence, the first thought for scaling would be
    to separate the database from the backend into its own service. Lastly, since we are dealing with heavier write than read loads, 
    (since a user on average generates way more newer content than accesses of previously generated content) 
    we can further scale the database service by using a master-slave config,
    where we delegate write tasks only to the master (maybe a multi-master config?) 
    and read tasks to the slaves (eventual consistency seems acceptable here? since we are already caching the latest data).

(I tried roughly sketching the idea on the diagram above ^^)


Thanks for reading, I'd love to hear your thoughts!
